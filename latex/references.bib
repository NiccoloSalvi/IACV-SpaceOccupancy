@Article{app15095170,
    AUTHOR = {Liu, Ye and Li, Shixin and Zhou, Liming and Liu, Haichen and Li, Zhiyu},
    TITLE = {Dark-YOLO: A Low-Light Object Detection Algorithm Integrating Multiple Attention Mechanisms},
    JOURNAL = {Applied Sciences},
    VOLUME = {15},
    YEAR = {2025},
    NUMBER = {9},
    ARTICLE-NUMBER = {5170},
    URL = {https://www.mdpi.com/2076-3417/15/9/5170},
    ISSN = {2076-3417},
    ABSTRACT = {Object detection in low-light environments is often hampered by unfavorable factors such as low brightness, low contrast, and noise, which lead to issues like missed detections and false positives. To address these challenges, this paper proposes a low-light object detection algorithm named Dark-YOLO, which dynamically extracts features. First, an adaptive image enhancement module is introduced to restore image information and enrich feature details. Second, the spatial feature pyramid module is improved by incorporating cross-overlapping average pooling and max pooling to extract salient features while retaining global and local information. Then, a dynamic feature extraction module is designed, which combines partial convolution with a parameter-free attention mechanism, allowing the model to flexibly capture critical and effective information from the image. Finally, a dimension reciprocal attention module is introduced to ensure the model can comprehensively consider various features within the image. Experimental results show that the proposed model achieves an mAP@50 of 71.3% and an mAP@50-95 of 44.2% on the real-world low-light dataset ExDark, demonstrating that Dark-YOLO effectively detects objects under low-light conditions. Furthermore, facial recognition in dark environments is a particularly challenging task. Dark-YOLO demonstrates outstanding performance on the DarkFace dataset, achieving an mAP@50 of 49.1% and an mAP@50-95 of 21.9%, further validating its effectiveness for face detection under complex low-light conditions.},
    DOI = {10.3390/app15095170}
}

@misc{hong2024lookaroundlearningillumination,
      title={You Only Look Around: Learning Illumination Invariant Feature for Low-light Object Detection}, 
      author={Mingbo Hong and Shen Cheng and Haibin Huang and Haoqiang Fan and Shuaicheng Liu},
      year={2024},
      eprint={2410.18398},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.18398}, 
}

@article{doi:10.1049/ipr2.13017,
    author = {Yun Xiao  and Hai Liao },
    title = {LIDA‐YOLO: An unsupervised low‐illumination object detection based on domain adaptation},
    journal = {IET Image Processing},
    volume = {18},
    issue = {5},
    pages = {1178-1188},
    year = {2024},
    doi = {10.1049/ipr2.13017},
    
    URL = {https://digital-library.theiet.org/doi/abs/10.1049/ipr2.13017},
    eprint = {https://digital-library.theiet.org/doi/pdf/10.1049/ipr2.13017}
    ,
        abstract = { The low‐light environment is integral to everyday activities but poses significant challenges in object detection. Due to the low brightness, noise, and insufficient illumination of the acquired image, the model's object detection performance is reduced. Opposing recent studies mainly developing using supervised learning models, this paper suggests LIDA‐YOLO, an approach for unsupervised adaptation of low‐illumination object detectors. The model improves the YOLOv3 by using normal illumination images as the source domain and low‐illumination images as the target domain and achieves object detection in low‐illumination images through an unsupervised learning strategy. Specifically, a multi‐scale local feature alignment and global feature alignment module are proposed to align the overall attributes of the image and feature biases such as background, scene, and target layout are thus reduced. The experimental results of LIDA‐YOLO on the ExDark dataset achieved the highest performance mAP score of 56.65\% compared to several current state‐of‐the‐art unsupervised domain adaptation object detection methods. Compared to I3Net, the performance improvement is 4.04\%, and compared to OSHOT, the performance improvement is 6.5\%. LIDA‐YOLO achieves a performance improvement of 2.7\% compared to the supervised baseline method YOLOv3. Overall, the suggested LIDA‐YOLO model requires fewer samples and presents a stronger generalization ability than previous works. LIDA‐YOLO, which improves the general object detection network YOLOv3 are proposed by using normal illumination images as the source domain and low‐illumination images as the target domain. LIDA‐YOLO can detect objects in low‐illumination images efficiently through unsupervised adaption, significantly reducing the model's dependence on samples.
    
    
    image
    image
     }
}

@misc{yang2024enhancingnighttimevehicledetection,
      title={Enhancing Nighttime Vehicle Detection with Day-to-Night Style Transfer and Labeling-Free Augmentation}, 
      author={Yunxiang Yang and Hao Zhen and Yongcan Huang and Jidong J. Yang},
      year={2024},
      eprint={2412.16478},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.16478}, 
}

@article{Zhang2024S_HOG,
    title = {{Vision‑Based On‑Road Nighttime Vehicle Detection and Tracking Using Improved HOG Features}},
    author = {Zhang, Li and Xu, Weiyue and Shen, Cong and Huang, Yingping},
    journal = {Sensors},
    volume = {24},
    number = {5},
    pages = {1590},
    year = {2024},
    doi = {10.3390/s24051590}
}

@inproceedings{Ali2020IPM,
    author       = {Ali, Muhammad and others},
    title        = {Real‑time vehicle distance estimation using single view geometry},
    booktitle    = {Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV)},
    year         = {2020},
    pages        = {xxx‑xxx},
    note         = {Section 3.2.1 describes IPM for perspective distortion removal},
}

@article{Zou2017HeadlightPairing,
  author    = {Zou, Q. and Ling, H. and Pang, Y. and Huang, Y. and Tian, M.},
  title     = {Joint Head‑light Pairing and Vehicle Tracking by Weighted Set Packing in Nighttime Traffic Videos},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  year      = {2017},
  volume    = {19},
  pages     = {1950--1961},
  doi       = {10.1109/TITS.2017.2745684}
}

@inproceedings{yin2022peyolo,
author    = {Yin, Wen and Xu, Jian and Lee, Dong and Park, Hyun},
title     = {PE-YOLO: Pyramid Enhancement for Low-Light Object Detection},
booktitle = {European Conference on Computer Vision},
year      = {2022},
pages     = {512--528}
}

@inproceedings{satzoda2019lightpair,
author    = {Satzoda, R. K. and Trivedi, M. M.},
title     = {Symmetry-Based Vehicle Detection and Distance Estimation Using Rear Lights},
booktitle = {International Conference on Intelligent Transportation Systems},
year      = {2019},
pages     = {2347--2354}
}

@inproceedings{zhang2022bytetrack,
author    = {Zhang, Yifu and Wang, Xiaojuan and Wang, Wei-Lin and Liu, Xue},
title     = {ByteTrack: Multi-Object Tracking by Associating Every Detection},
booktitle = {European Conference on Computer Vision},
year      = {2022},
pages     = {1--17}
}

@inproceedings{wojke2017deepsort,
author    = {Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
title     = {Simple Online and Realtime Tracking with a Deep Association Metric},
booktitle = {International Conference on Image Processing},
year      = {2017},
pages     = {3645--3649}
}

@article{he2024lightclustering,
author    = {He, Xin and Li, Jun and Zhang, Lei and Zhao, Peng},
title     = {Headlight Clustering for Robust Nighttime Vehicle Tracking},
journal   = {Expert Systems with Applications},
year      = {2024},
volume    = {214},
pages     = {119123}
}

@article{rezaei2023zerocalib,
author    = {Rezaei, Arash and Yin, Xia and Bai, Feng},
title     = {Zero-Calibration Monocular 3D Vehicle Localization via Satellite-Ground Alignment},
journal   = {IEEE Transactions on Intelligent Vehicles},
year      = {2023},
volume    = {8},
number    = {3},
pages     = {456--468}
}
