\section{Background and Motivation}
Vehicle space occupancy estimation represents a fundamental challenge in modern intelligent transportation systems, autonomous vehicle navigation, and traffic monitoring applications. The ability to accurately determine the spatial footprint of moving vehicles from visual data is crucial for collision avoidance, path planning, parking assistance, and traffic flow analysis. Traditional approaches often rely on complex sensor fusion systems, expensive LiDAR technology, or require extensive computational resources that limit real-time application feasibility.
The challenge becomes particularly pronounced in low-light conditions where conventional computer vision algorithms struggle to maintain accuracy. Nighttime driving scenarios, tunnel environments, and poorly illuminated areas represent critical operational contexts where robust vehicle detection and space estimation are essential for safety-critical applications. These conditions necessitate specialized approaches that can leverage available visual cues, such as vehicle lighting systems, to maintain reliable performance.

\section{Related Work}
Vehicle detection and tracking under nighttime conditions using a single calibrated camera has emerged as a challenging yet increasingly active area of research, driven by its relevance to intelligent transportation systems and autonomous navigation. Existing literature can be broadly categorized into four main areas: deep-learning-based detectors adapted to low-light environments, traditional light-based methods, monocular 3D localization techniques, and multi-frame tracking algorithms.

In the realm of deep learning, recent works have introduced architectural enhancements to object detectors specifically designed to operate in low-light scenarios. Dark-YOLO, proposed by Liu et al.\cite{app15095170}, incorporates a trainable denoising and brightening module within the YOLO detection framework. By combining cross-scale feature pyramids and attention mechanisms, it achieves 71.3\% mAP@50 on the ExDark dataset while maintaining real-time inference capability. Similarly, YOLA by Hong et al.\cite{hong2024lookaroundlearningillumination} introduces a Lambertian reflectance-based feature extractor to enforce illumination invariance directly within the backbone network. This method avoids explicit image preprocessing while significantly improving recall in nighttime detection tasks. Xiao et al.\cite{doi:10.1049/ipr2.13017} present LIDA-YOLO, which applies unsupervised domain adaptation from daytime to nighttime imagery. By aligning multi-scale feature distributions across lighting conditions, their model generalizes effectively to dark environments without requiring manually labeled night-time data. Another relevant contribution is from Yang et al.\cite{yang2024enhancingnighttimevehicledetection}, who propose a GAN-based style transfer pipeline that transforms daytime images into synthetic nighttime variants. By training on a mixture of real and stylized images—including synthetic scenes rendered in CARLA—the augmented dataset enhances the robustness of conventional YOLO detectors to night scenes.

Prior to the dominance of deep learning, classical vision pipelines exploited geometric and photometric properties of vehicle lighting. Xu et al.\cite{Zhang2024S_HOG} propose a multi-stage pipeline that first segments salient bright regions and then verifies them using superpixel-weighted HOG descriptors classified by a support vector machine (SVM). A Kalman filter is then used for temporal association. This method proves effective in scenarios where only rear light pairs are visible. Satzoda and Trivedi\cite{satzoda2019lightpair} develop a light-pair detector based on Haar-like features and AdaBoost. Their approach uses camera calibration to reject false detections based on physical spacing constraints between detected lights and further infers approximate vehicle distance.

Monocular methods for 3D localization address the challenge of mapping image-based detections onto the real-world road plane. A common approach is inverse perspective mapping (IPM), which, given a known camera calibration, warps 2D bounding boxes into bird’s-eye view projections, enabling consistent estimation of object dimensions and positions\cite{Ali2020IPM}. Rezaei et al.\cite{rezaei2023zerocalib} further propose a zero-calibration method that leverages publicly available map data to automatically infer extrinsic parameters. This enables monocular localization and perspective correction without requiring manual setup or ground-plane constraints.

Finally, multi-frame tracking approaches have been widely adopted to refine noisy detections and ensure temporal consistency in vehicle localization. Zhang et al.\cite{zhang2022bytetrack} introduce ByteTrack, a tracker that retains low-confidence detections during data association using motion-based intersection over union (IoU), significantly reducing the rate of track fragmentation in challenging nighttime sequences. DeepSORT, proposed by Wojke et al.\cite{wojke2017deepsort}, extends classical Kalman filtering by incorporating deep appearance features, thus enabling more reliable tracking under occlusions and varying illumination. More recently, He et al.\cite{Zou2017HeadlightPairing} propose a probabilistic clustering method that models pairs of headlights using Gaussian mixture models over time, achieving multiple object tracking accuracy (MOTA) scores exceeding 90\% on nighttime highway videos.

In summary, the current state-of-the-art in nighttime vehicle detection and tracking involves an integrated pipeline consisting of low-light-adapted detectors, robust multi-object trackers, and geometric projection techniques to estimate real-world occupancy. Despite recent advances, challenges persist in generalizing across diverse lighting conditions, handling ambiguous feature visibility, and achieving reliable 3D localization in the absence of strong perspective cues.
 

\section{Problem Statement}
This project addresses the specific challenge of developing a vision-based system capable of analyzing videos of moving vehicles captured by a fixed camera to draw a rectangular parallelepiped bounding box around the vehicle for each frame under low-light conditions. The primary objective is not to determine the actual space occupied by the car, but rather to accurately estimate and visualize the 3D bounding volume that encompasses the entire vehicle using geometric reconstruction techniques. The system must operate with minimal prior information, requiring only the intrinsic calibration parameters of the camera ($\mathbf{K}$ matrix) and a simplified model of the observed vehicle including basic dimensional parameters such as length, width, and the spatial configuration of rear lights.

\section{Assumptions}
The system operates under the following assumptions:
\begin{itemize}
    \item The camera is static and intrinsically calibrated (calibration matrix $K$ is known).
    \item The car has a vertical symmetry plane and two symmetric rear lights that are visible in the scene.
    \item The road is locally planar.
    \item Between consecutive frames, the vehicle either translates forward or follows a motion with constant curvature.
\end{itemize}

\section*{Implementation and Resources}
The implementation developed for this project closely follows the methodology illustrated in specific slides provided by the course instructor. Both the full working code and the reference slides are available in the following public GitHub repository.\footnote{\url{https://github.com/NiccoloSalvi/IACV-SpaceOccupancy}}